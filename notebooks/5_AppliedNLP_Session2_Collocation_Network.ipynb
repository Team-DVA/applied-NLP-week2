{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a65cb4e7",
   "metadata": {},
   "source": [
    "# 5) Collocation Network\n",
    "\n",
    "This notebook is part of **Applied NLP – Session 2: Phrases & Collocations**.\n",
    "\n",
    "Overview:\n",
    "- Visualize word collocations as a network graph where nodes are words and edges are bigram co-occurrences.\n",
    "- Identify hub words (high degree) that co-occur frequently with many other words.\n",
    "- Filter edges by frequency threshold to focus on strong collocations and reveal phrase structure.\n",
    "\n",
    "Learning objectives:\n",
    "- Represent bigram data as a network graph using NetworkX.\n",
    "- Compute and visualize largest connected components to see collocation clusters.\n",
    "- Interpret node degree as a measure of word centrality in phrasal patterns.\n",
    "- Use network visualization to explore stylistic and thematic phrase structure.\n",
    "\n",
    "Quick start:\n",
    "1. Edit the `CONFIG` dictionary in the next code cell to point to your two plain-text books.\n",
    "2. Adjust `min_ngram_count` to control edge density (higher = sparser graph, clearer hubs).\n",
    "3. (Optional) Toggle `use_stopwords` to remove function words and focus on content-word collocations.\n",
    "4. Run cells from top to bottom. The main outputs are saved to `../results/`.\n",
    "\n",
    "Prerequisites:\n",
    "- A Python environment with requirements.txt packages installed (pandas, matplotlib, networkx).\n",
    "- The text files for the two works placed in `../data/`.\n",
    "\n",
    "Notes and tips:\n",
    "- The notebook uses the same robust preprocessing as notebooks 1-2 (strip_gutenberg, normalize quotes, etc.).\n",
    "- Network graph: each word is a node; each bigram (a, b) creates an edge between nodes a and b weighted by co-occurrence count.\n",
    "- Self-loops (edges from a word to itself) are filtered out for clarity.\n",
    "- Largest connected component is extracted to avoid isolated word pairs cluttering the visualization.\n",
    "- Node size scales with degree (number of connections); hub words appear larger.\n",
    "- Edge width scales with weight (bigram count); frequent collocations appear thicker.\n",
    "- Spring layout: positions nodes to minimize edge crossings and reveal structure.\n",
    "- Consider removing stopwords first or filtering to content-words (nouns, verbs, adjectives) for cleaner networks.\n",
    "- Compare per-book networks by running the same code on `text1` and `text2` separately to see shifts in collocation patterns.\n",
    "\n",
    "**Goal:** Build and visualize a collocation network to explore phrase structure and identify hub words in your two selected works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609e5a1a",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "\n",
    "- Fill the `CONFIG` paths for your two books (plain text).\n",
    "- Toggle stopwords and thresholds as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65431f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports & Config =====\n",
    "import re, os, math, json, collections\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"..\\\\data\\\\Fellowship.txt\",\n",
    "    \"book2_path\": \"..\\\\data\\\\TwoTowers.txt\",\n",
    "    \"book3_path\": \"..\\\\data\\\\TheKing.txt\",\n",
    "    \"language\": \"en\",                # e.g. 'en','de','ru','el'\n",
    "    \"use_stopwords\": False,          # toggle\n",
    "    \"min_ngram_count\": 5,            # threshold (where applicable)\n",
    "    \"top_k\": 20                      # top items to show\n",
    "}\n",
    "\n",
    "# Unicode-aware token regex: words with optional internal ' or -\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:[-'][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "# Optional: supply your own stopwords set per language\n",
    "STOPWORDS = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11547f59",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "\n",
    "- Fix hyphenated line breaks (e.g., end-of-line hyphens).\n",
    "- Normalize whitespace.\n",
    "- Lowercase consistently.\n",
    "\n",
    "Our books are a part of Project Gutenberg, which means there are some extra texts in each txt file to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0187e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust Project Gutenberg boilerplate stripper --------------------------\n",
    "_GB_START_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",   # modern\n",
    "    r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",             # fallback\n",
    "    r\"End of the Project Gutenberg(?:'s)? Etext\",               # very old variants sometimes inverted\n",
    "]\n",
    "_GB_END_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",      # modern\n",
    "    r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",                # fallback\n",
    "    r\"End of Project Gutenberg(?:'s)? (?:Etext|eBook)\",          # older variants\n",
    "    r\"\\*\\*\\*\\s*END: FULL LICENSE\\s*\\*\\*\\*\",                      # license block end (older)\n",
    "]\n",
    "\n",
    "# Chapters (heuristic fallback if markers missing; English-centric but works often)\n",
    "_CHAPTER_HINTS = [\n",
    "    r\"^\\s*chapter\\s+[ivxlcdm0-9]+[\\.\\: ]\",   # CHAPTER I / Chapter 1\n",
    "    r\"^\\s*book\\s+[ivxlcdm0-9]+[\\.\\: ]\",      # BOOK I etc.\n",
    "    r\"^\\s*part\\s+[ivxlcdm0-9]+[\\.\\: ]\",\n",
    "]\n",
    "\n",
    "def strip_gutenberg(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns text between Gutenberg START and END markers (case-insensitive).\n",
    "    If markers aren't found, heuristically trims to first chapter-like heading.\n",
    "    Works for most EN/DE/RU/EL releases since headers are in English.\n",
    "    \"\"\"\n",
    "    t = text.replace(\"\\ufeff\", \"\")  # strip BOM if present\n",
    "\n",
    "    # Find START\n",
    "    start_idx = None\n",
    "    for pat in _GB_START_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # start AFTER the matched line\n",
    "            start_idx = t.find(\"\\n\", m.end())\n",
    "            if start_idx == -1:\n",
    "                start_idx = m.end()\n",
    "            break\n",
    "\n",
    "    # Find END\n",
    "    end_idx = None\n",
    "    for pat in _GB_END_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # end BEFORE the matched line\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "\n",
    "    if start_idx is not None and end_idx is not None and end_idx > start_idx:\n",
    "        core = t[start_idx:end_idx]\n",
    "    else:\n",
    "        # Fallback: try to start at first chapter-like heading\n",
    "        core = t\n",
    "        for pat in _CHAPTER_HINTS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE | re.MULTILINE)\n",
    "            if m:\n",
    "                core = core[m.start():]\n",
    "                break\n",
    "        # And trim off the standard license tail if present\n",
    "        for pat in _GB_END_MARKERS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                core = core[:m.start()]\n",
    "                break\n",
    "\n",
    "    # Remove license/contact blocks that sometimes sneak inside\n",
    "    core = re.sub(r\"\\n\\s*End of the Project Gutenberg.*\", \"\", core, flags=re.IGNORECASE)\n",
    "    core = re.sub(r\"\\*\\*\\*\\s*START: FULL LICENSE\\s*\\*\\*\\*.*\", \"\", core, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Clean leftover cruft: URLs, repeated separators\n",
    "    core = re.sub(r\"https?://\\S+\", \"\", core)\n",
    "    core = re.sub(r\"[ \\t]+\\n\", \"\\n\", core)   # trailing spaces before newline\n",
    "    core = re.sub(r\"\\n{3,}\", \"\\n\\n\", core)   # collapse big blank blocks\n",
    "    return core.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42845e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) strip Gutenberg header/footer FIRST\n",
    "    t = strip_gutenberg(t)\n",
    "    # 2) join hyphenated line breaks (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "len(tokens1), len(tokens2), len(tokens)\n",
    "\n",
    "len(tokens), tokens[:12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b1399b",
   "metadata": {},
   "source": [
    "## 2. Build Bigram Graph\n",
    "\n",
    "- Nodes: words\n",
    "- Edges: bigrams (weight = co-occurrence count)\n",
    "- Filter edges by `min_ngram_count` for clarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "bigrams_counts = Counter(zip(tokens, tokens[1:]))\n",
    "min_c = CONFIG[\"min_ngram_count\"]\n",
    "edges = [(a,b,c) for (a,b), c in bigrams_counts.items() if c >= min_c and a != b]\n",
    "\n",
    "G = nx.Graph()\n",
    "for a,b,c in edges:\n",
    "    G.add_edge(a, b, weight=c)\n",
    "\n",
    "# Keep largest connected component for readability\n",
    "if G.number_of_nodes() > 0:\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    H = G.subgraph(largest_cc).copy()\n",
    "else:\n",
    "    H = G\n",
    "\n",
    "len(G), len(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb26a26",
   "metadata": {},
   "source": [
    "## 3. Visualize Graph (spring layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a643a0",
   "metadata": {},
   "source": [
    "## 3a. Filtered Network (Top Hubs Only)\n",
    "\n",
    "For a clearer visualization, we aggressively filter to show only:\n",
    "- **Top hub words** (highest degree nodes)\n",
    "- **Strong connections** between these hubs only (edge weight ≥ 2× min_ngram_count)\n",
    "\n",
    "This reveals the core collocation structure without overcrowding. You can adjust:\n",
    "- `top_n_hubs` - how many hub words to show (default: 20)\n",
    "- `min_edge_weight` - minimum connection strength to display (default: 2× min_ngram_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66454f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strategy: Show only strongest edges between high-degree nodes\n",
    "# This creates a much cleaner, more readable network\n",
    "\n",
    "# 1. Select top hubs by degree\n",
    "top_n_hubs = 50  # start with top 20 hub words\n",
    "deg = dict(H.degree())\n",
    "top_hubs = set(sorted(deg, key=deg.get, reverse=True)[:top_n_hubs])\n",
    "\n",
    "# 2. Filter to only include edges between these hubs with strong connections\n",
    "# Only keep edges where BOTH nodes are hubs AND edge weight is high\n",
    "edge_weights = [(u, v, d['weight']) for u, v, d in H.edges(data=True)]\n",
    "edge_weights.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Keep only edges where both nodes are in top hubs and weight is above threshold\n",
    "min_edge_weight = CONFIG[\"min_ngram_count\"] * 2  # stricter threshold for cleaner graph\n",
    "filtered_edges = [\n",
    "    (u, v, w) for u, v, w in edge_weights \n",
    "    if u in top_hubs and v in top_hubs and w >= min_edge_weight\n",
    "]\n",
    "\n",
    "# 3. Build clean filtered graph\n",
    "H_filtered = nx.Graph()\n",
    "for u, v, w in filtered_edges:\n",
    "    H_filtered.add_edge(u, v, weight=w)\n",
    "\n",
    "print(f\"Filtered network: {H_filtered.number_of_nodes()} nodes, {H_filtered.number_of_edges()} edges\")\n",
    "\n",
    "# 4. Visualize - much cleaner now!\n",
    "fig_filtered = plt.figure(figsize=(14,10))\n",
    "pos = nx.spring_layout(H_filtered, k=0.8, iterations=100, seed=42)\n",
    "deg_filtered = dict(H_filtered.degree())\n",
    "wts_filtered = [H_filtered[u][v][\"weight\"] for u,v in H_filtered.edges()]\n",
    "\n",
    "if wts_filtered and H_filtered.number_of_nodes() > 0:\n",
    "    wmin, wmax = min(wts_filtered), max(wts_filtered)\n",
    "    ew_filtered = [1 + 4*(w - wmin)/(wmax - wmin + 1e-9) for w in wts_filtered]\n",
    "    \n",
    "    # Draw nodes with size based on degree\n",
    "    node_sizes = [200 + 50*deg_filtered[n] for n in H_filtered]\n",
    "    nx.draw_networkx_nodes(H_filtered, pos, node_size=node_sizes, alpha=0.85, node_color='lightblue', edgecolors='steelblue', linewidths=2)\n",
    "    nx.draw_networkx_edges(H_filtered, pos, width=ew_filtered, alpha=0.5, edge_color='gray')\n",
    "    \n",
    "    # Label all nodes - now readable!\n",
    "    nx.draw_networkx_labels(H_filtered, pos, font_size=12, font_weight='bold')\n",
    "    \n",
    "    plt.title(f\"Collocation Network - Top Hub Words (min edge weight ≥ {min_edge_weight})\")\n",
    "    plt.axis(\"off\"); plt.tight_layout(); plt.show()\n",
    "else:\n",
    "    print(f\"Graph too sparse. Try lowering min_edge_weight (currently {min_edge_weight}) or increase top_n_hubs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063f46fe",
   "metadata": {},
   "source": [
    "Can you now filter for stopwords and look again at the network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c487a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_network = plt.figure(figsize=(10,8))\n",
    "pos = nx.spring_layout(H, k=0.25, iterations=50, seed=42)\n",
    "deg = dict(H.degree())\n",
    "wts = [H[u][v][\"weight\"] for u,v in H.edges()]\n",
    "\n",
    "if wts:\n",
    "    wmin, wmax = min(wts), max(wts)\n",
    "    ew = [0.5 + 2.5*(w - wmin)/(wmax - wmin + 1e-9) for w in wts]\n",
    "else:\n",
    "    ew = []\n",
    "\n",
    "nx.draw_networkx_nodes(H, pos, node_size=[24 + 6*deg[n] for n in H], alpha=0.85)\n",
    "nx.draw_networkx_edges(H, pos, width=ew, alpha=0.35)\n",
    "\n",
    "# Label a few hubs\n",
    "top_nodes = sorted(deg, key=deg.get, reverse=True)[:20]\n",
    "nx.draw_networkx_labels(H, pos, labels={n:n for n in top_nodes}, font_size=9)\n",
    "plt.title(f\"Collocation Network (min bigram count ≥ {min_c})\")\n",
    "plt.axis(\"off\"); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe91c2c",
   "metadata": {},
   "source": [
    "## 4. Notes\n",
    "\n",
    "- Consider removing stopwords or filtering to content-words first.\n",
    "- Compare per-book networks to see shifts in phrase structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee527f7",
   "metadata": {},
   "source": [
    "## 5. Reflection (Answer in your repo's README or below)\n",
    "\n",
    "- Which results matched your reading intuition?\n",
    "- What surprised you?\n",
    "- If you toggled preprocessing (stopwords on/off), what changed?\n",
    "- Compare across the two works: are the patterns stable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f59f5ad",
   "metadata": {},
   "source": [
    "## 6. Export (tables/figures)\n",
    "\n",
    "This cell saves outputs into the `../results/` folder so you can add them to your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d52b174",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../results\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save full network figure\n",
    "try:\n",
    "    fig_network.savefig(\"../results/collocation_network_full.png\", dpi=200, bbox_inches=\"tight\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "# Save filtered network figure (clearer, more readable)\n",
    "try:\n",
    "    fig_filtered.savefig(\"../results/collocation_network_filtered.png\", dpi=200, bbox_inches=\"tight\")\n",
    "except NameError:\n",
    "    try:\n",
    "        plt.savefig(\"../results/collocation_network_filtered.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    except Exception:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
